0
SLURM_JOBID=4796
SLURM_JOB_NODELIST=iitrdgx
SLURM_NNODES=1
SLURMTMPDIR=
Date              = Sun May 30 18:10:07 IST 2021
Hostname          = iitrdgx

Number of Nodes Allucated      = 1
Number of Tasks Allocated      = 1
Number of Cores/Task Allocated = 4
Working Directory = /raid/ysharma_me/fair_lr/LfF
working directory = /raid/ysharma_me/fair_lr/LfF
/raid/ysharma_me/fair_lr/LfF
iitrdgx
Sun May 30 18:10:07 IST 2021
0

=============
== PyTorch ==
=============

NVIDIA Release 20.11 (build 17345815)
PyTorch Version 1.8.0a0+17f8c32

Container image Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.

Copyright (c) 2014-2020 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.
NVIDIA modifications are covered by the license terms that apply to the underlying project or file.

NOTE: Legacy NVIDIA Driver detected.  Compatibility mode ENABLED.

ERROR: Detected MOFED driver 4.0-1.0.1, but this container has version 5.1-2.3.7.
       Please upgrade to MOFED 4.9 or higher for multi-node operation support.

Matplotlib created a temporary config/cache directory at /tmp/matplotlib-njb2ns9j because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
[12:40:13] [WARNING] [debias.main] No observers have been added to this run
[12:40:14] [INFO] [debias.main] Running command 'train'
[12:40:14] [INFO] [debias.main] Started
CelebA
Downloading: "https://download.pytorch.org/models/resnet18-5c106cde.pth" to raid/ysharma_me/fair_lr/dnew/checkpoints/resnet18-5c106cde.pth
  0% 0.00/44.7M [00:00<?, ?B/s] 15% 6.58M/44.7M [00:00<00:00, 68.9MB/s] 38% 16.8M/44.7M [00:00<00:00, 77.2MB/s] 63% 28.0M/44.7M [00:00<00:00, 84.7MB/s] 88% 39.2M/44.7M [00:00<00:00, 92.5MB/s]100% 44.7M/44.7M [00:00<00:00, 104MB/s] 
  0% 0/127200 [00:00<?, ?it/s]  0% 0/127200 [00:03<?, ?it/s]
[12:40:25] [ERROR] [debias.main] Failed after 0:00:11!
Traceback (most recent calls WITHOUT Sacred internals):
  File "/raid/ysharma_me/fair_lr/LfF/train_dnew.py", line 277, in train
    Dlogit = D(feat)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/raid/ysharma_me/fair_lr/LfF/attention.py", line 49, in forward
    t=self.fc1(t)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py", line 1370, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: size mismatch, m1: [256 x 2], m2: [1024 x 128] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290

Exception in thread Thread-6:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/opt/conda/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py", line 25, in _pin_memory_loop
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/opt/conda/lib/python3.6/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 294, in rebuild_storage_fd
    fd = df.detach()
  File "/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/opt/conda/lib/python3.6/multiprocessing/connection.py", line 493, in Client
    answer_challenge(c, authkey)
  File "/opt/conda/lib/python3.6/multiprocessing/connection.py", line 732, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/opt/conda/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/opt/conda/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/opt/conda/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer

